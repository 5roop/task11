{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2577880023f865b3\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-2577880023f865b3/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a856eb4eb6d242009e0fe99a67022a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_dataset\n",
    "\n",
    "output_column = \"Speaker_gender\"\n",
    "model_name_or_path = \"./models/facebook_wav2vec2-large-slavic-voxpopuli-v2gender_2s_0/checkpoint-250\"\n",
    "test_dataset = load_dataset(\"csv\", data_files={\"test\": \"002_gender_test_for_datasets.csv\"}, delimiter=\",\")[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:52: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "try:\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n",
    "except:\n",
    "    from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "    tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\n",
    "        \"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\" \"\n",
    "    )\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1,\n",
    "        sampling_rate=16000,\n",
    "        padding_value=0.0,\n",
    "        do_normalize=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    speech_array = speech_array.squeeze().numpy()\n",
    "    speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, processor.feature_extractor.sampling_rate)\n",
    "\n",
    "    batch[\"speech\"] = speech_array\n",
    "    return batch\n",
    "\n",
    "\n",
    "def predict(batch):\n",
    "    features = processor(batch[\"speech\"], sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits \n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    batch[\"predicted\"] = pred_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950ec75e462b431886240db824e451e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5bcedee8274e4f8bc7a21ab96d79b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "result = test_dataset.map(predict, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 'M']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = [config.id2label[i] for i in range(config.num_labels)]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "y_true = [config.label2id[name] for name in result[output_column]]\n",
    "y_pred = result[\"predicted\"]\n",
    "\n",
    "print(y_true[:5])\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "y_true = [name for name in result[output_column]]\n",
    "y_pred = [config.id2label[name] for name in result[\"predicted\"]]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"science no-latex\".split())\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "fi, ax = plt.subplots(figsize=(5,5))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "   y_true, y_pred, cmap=\"Oranges\", ax=ax, xticks_rotation=90)\n",
    "plt.savefig(\"images/003_cm_slavic_speaker_gender_2s.png\")\n",
    "plt.savefig(\"images/003_cm_slavic_speaker_gender_2s.pdf\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8915"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
