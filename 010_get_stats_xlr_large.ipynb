{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import example_eval_config, example_train_config, eval_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4e4c237abd178dbd\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-4e4c237abd178dbd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5108abde2254fa5b7eb9de5d6fcb2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:52: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-4e4c237abd178dbd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-88f43f5345a6720b.arrow\n",
      "Parameter 'function'=<function eval_model.<locals>.predict at 0x7fa4b630f9d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8242305c46ae4b0c83160e8e40c3105c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-76a9fdba68e7ac11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/peterr/.cache/huggingface/datasets/csv/default-76a9fdba68e7ac11/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8837487f5a490d80f2044d22ee5ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc239810ee1f46d78ee51d6f9634dbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/peterr/.cache/huggingface/datasets/csv/default-76a9fdba68e7ac11/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92260496d3ef4a9e83aa3f1f73cf0c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb4e6fc9e7b4ad2ba5d2f997b1ab713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9143f7949ec4e7eb0bd9cdf7f1a53d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# No clipping:\n",
    "clip = -1\n",
    "OUTPUT_DIR = \"models/facebook_wav2vec2-large_NEW_GENDER_\" \n",
    "if clip == 2:\n",
    "    OUTPUT_DIR += \"2s_\"\n",
    "\n",
    "for split in \"dev test\".split():\n",
    "    config = {\n",
    "        \"output_column\": \"Speaker_gender\",\n",
    "        \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-250\",\n",
    "        \"eval_file\": f\"001_gender_{split}.csv\",\n",
    "        \"clip_seconds\": clip,\n",
    "    }\n",
    "    y_true, y_pred = eval_model(config)\n",
    "\n",
    "    results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4e4c237abd178dbd\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-4e4c237abd178dbd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72324442d87c4147b9a9fd0c8637dfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-4e4c237abd178dbd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-88f43f5345a6720b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9437859938647bba618a34f5833f3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-76a9fdba68e7ac11\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-76a9fdba68e7ac11/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7497dedd92844835a4280f1e60673f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-76a9fdba68e7ac11/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-e83424028abbf45d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c69e65ab9c14c5c8f462ee94769d315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data=results).to_json(\"010_results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# Yes clipping:\n",
    "clip = 2\n",
    "OUTPUT_DIR = \"models/facebook_wav2vec2-large_NEW_GENDER_\" \n",
    "if clip == 2:\n",
    "    OUTPUT_DIR += \"2s_\"\n",
    "\n",
    "for split in \"dev test\".split():\n",
    "    config = {\n",
    "        \"output_column\": \"Speaker_gender\",\n",
    "        \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-250\",\n",
    "        \"eval_file\": f\"001_gender_{split}.csv\",\n",
    "        \"clip_seconds\": clip,\n",
    "    }\n",
    "    y_true, y_pred = eval_model(config)\n",
    "\n",
    "    results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: speaker ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f0acc4cba1c49a4a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/peterr/.cache/huggingface/datasets/csv/default-f0acc4cba1c49a4a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6faef370b7a44b9a090a3b506c4acfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2527e916ccf4e3394c81528fca08b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/peterr/.cache/huggingface/datasets/csv/default-f0acc4cba1c49a4a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15feb1a9d1234fdeb8e394ba18183a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6fc6da77694085a138fb7d296ff62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53267e7e187d4e1daee66ab55d5a2fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c0c2a995251c9abb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/peterr/.cache/huggingface/datasets/csv/default-c0c2a995251c9abb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcbe6a4d71d4eda8e681baae343f55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8715546fe5a44322a422a1635142a43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/peterr/.cache/huggingface/datasets/csv/default-c0c2a995251c9abb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bd80f2a6df4c4f9d58283f5937218b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417e3c05edad47a1be6ba150c75f7f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94166e9045814047ad875005914f1908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f0acc4cba1c49a4a\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-f0acc4cba1c49a4a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369bdd84774847fb88b8149792211051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-f0acc4cba1c49a4a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-9b1013a4ed972841.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006ea97a807e498eb120dca808e94e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c0c2a995251c9abb\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-c0c2a995251c9abb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c70f797653a444eb733fc95c7a14fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-c0c2a995251c9abb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-eb3599ed6b9a654e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a1ba5fee9f4712ac2f8dfd3be41ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data=results).to_json(\"010_results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "clip = 2\n",
    "OUTPUT_DIR = \"models/facebook_wav2vec2-large_NEW_SPEAKER_ID_\" \n",
    "if clip == 2:\n",
    "    OUTPUT_DIR += \"2s_\"\n",
    "\n",
    "for split in \"dev test\".split():\n",
    "    config = {\n",
    "        \"output_column\": \"Speaker_name\",\n",
    "        \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-1250\",\n",
    "        \"eval_file\": f\"003_speaker_id_{split}_for_datasets.csv\",\n",
    "        \"clip_seconds\": clip,\n",
    "    }\n",
    "    y_true, y_pred = eval_model(config)\n",
    "\n",
    "    results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(data=results).to_json(\"010_results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "clip = -1\n",
    "OUTPUT_DIR = \"models/facebook_wav2vec2-large_NEW_SPEAKER_ID_\" \n",
    "if clip == 2:\n",
    "    OUTPUT_DIR += \"2s_\"\n",
    "\n",
    "for split in \"dev test\".split():\n",
    "    config = {\n",
    "        \"output_column\": \"Speaker_name\",\n",
    "        \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-1250\",\n",
    "        \"eval_file\": f\"003_speaker_id_{split}_for_datasets.csv\",\n",
    "        \"clip_seconds\": clip,\n",
    "    }\n",
    "    y_true, y_pred = eval_model(config)\n",
    "\n",
    "    results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=results)\n",
    "df.to_json(\"010_results.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Age group - males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-00a104b1622251f1\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2666ccf744494809b2a6f0d23a093270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classification problem with 2 classes: ['old', 'young']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-48e0b8550720862e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-c83db76a4186f749.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-ce0be6df15d6386e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-ff21a037e26d0ab0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-3d934e6f96c42063.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-70419c9c0414430f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-a29ac06125c1a6ae.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-5861b23698647a83.arrow\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.codevectors', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1200\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 2250\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m5roop\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/peterr/macocu/task11/wandb/run-20220518_083433-11p4la43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/5roop/huggingface/runs/11p4la43\" target=\"_blank\">models/facebook_wav2vec2-large_NEW_AGE_ID_</a></strong> to <a href=\"https://wandb.ai/5roop/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1901' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1901/2250 2:35:37 < 28:36, 0.20 it/s, Epoch 12.67/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.748400</td>\n",
       "      <td>0.720709</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>0.700684</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.798813</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.703464</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.718100</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.715300</td>\n",
       "      <td>0.693115</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.711200</td>\n",
       "      <td>0.694580</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.717700</td>\n",
       "      <td>0.694351</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>0.693465</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.703400</td>\n",
       "      <td>0.739990</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.682900</td>\n",
       "      <td>0.696373</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.696964</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.715820</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.700300</td>\n",
       "      <td>0.699028</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.706600</td>\n",
       "      <td>0.693607</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.698200</td>\n",
       "      <td>0.698812</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.693359</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.740700</td>\n",
       "      <td>0.693413</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/250 01:48 < 01:43, 1.17 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-150\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-150/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-150/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-150/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-2250] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-300\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-300/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-150] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-450\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-450/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-450/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-450/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-600\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-600/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-450] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-750\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-750/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-750/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-750/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-900\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-900/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-900/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-900/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-750] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1050\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1050/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1050/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1050/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1200\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1200/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1050] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1350\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1350/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1350/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1350/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1500\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1500/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1500/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1350] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1650\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1650/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1650/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1650/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1800\n",
      "Configuration saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1800/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large_NEW_AGE_ID_/checkpoint-1650] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording. If Count, path, Speaker_age_group, Speaker_name, Speaker_gender, Speaker_age_at_recording are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    }
   ],
   "source": [
    "from utils import train_model\n",
    "for clip in [-1]:\n",
    "    train_config = {\n",
    "        'model_name_or_path': 'facebook/wav2vec2-large',\n",
    "        'TASK': 'NEW_AGE_ID_2s' if clip==2 else \"NEW_AGE_ID\",\n",
    "        'NUM_EPOCH': 15,\n",
    "        'data_files': {\n",
    "            'train': '006_age_train.csv',\n",
    "            'validation': '006_age_test.csv'\n",
    "            },\n",
    "        \"clip_seconds\": clip,\n",
    "        \"output_column\": \"Speaker_age_group\"\n",
    "    }\n",
    "\n",
    "    OUTPUT_DIR = train_model(train_config)\n",
    "    \n",
    "    for split in \"dev test\".split():\n",
    "        config = {\n",
    "            \"output_column\": \"Speaker_age_group\",\n",
    "            \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-2250\",\n",
    "            \"eval_file\": f\"006_age_{split}.csv\",\n",
    "            \"clip_seconds\": clip,\n",
    "            \"train_config\": train_config,\n",
    "        }\n",
    "        y_true, y_pred = eval_model(config)\n",
    "\n",
    "        results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(data=results)\n",
    "        df.to_json(\"010_results.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=results)\n",
    "df.to_json(\"010_results.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
