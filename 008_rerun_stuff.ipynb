{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import example_eval_config, example_train_config, eval_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4e4c237abd178dbd\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-4e4c237abd178dbd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822d583d258f41c8bde15329c6117fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:52: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-4e4c237abd178dbd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-88f43f5345a6720b.arrow\n",
      "Parameter 'function'=<function eval_model.<locals>.predict at 0x7fc36d6c4a60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c541ee33ff749539267b9f4ee7d7a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-76a9fdba68e7ac11\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-76a9fdba68e7ac11/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1024f3ffb6454252ae652280aa07c66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-76a9fdba68e7ac11/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-e83424028abbf45d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e64da9283e246a29922d9459199a50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# No clipping:\n",
    "clip = -1\n",
    "OUTPUT_DIR = \"models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_GENDER_\" \n",
    "if clip == 2:\n",
    "    OUTPUT_DIR += \"2s_\"\n",
    "\n",
    "for split in \"dev test\".split():\n",
    "    config = {\n",
    "        \"output_column\": \"Speaker_gender\",\n",
    "        \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-250\",\n",
    "        \"eval_file\": f\"001_gender_{split}.csv\",\n",
    "        \"clip_seconds\": clip,\n",
    "    }\n",
    "    y_true, y_pred = eval_model(config)\n",
    "\n",
    "    results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4e4c237abd178dbd\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-4e4c237abd178dbd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80956c782134771aa5afaca85047999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-4e4c237abd178dbd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-88f43f5345a6720b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7bda1eea6e40d1b97291af767d0a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-76a9fdba68e7ac11\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-76a9fdba68e7ac11/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0694646af04993a9c98680e2c05ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-76a9fdba68e7ac11/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-e83424028abbf45d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a53415c76c4ed294229af7eeea289b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data=results).to_json(\"008_results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# Yes clipping:\n",
    "clip = 2\n",
    "OUTPUT_DIR = \"models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_GENDER_\" \n",
    "if clip == 2:\n",
    "    OUTPUT_DIR += \"2s_\"\n",
    "\n",
    "for split in \"dev test\".split():\n",
    "    config = {\n",
    "        \"output_column\": \"Speaker_gender\",\n",
    "        \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-250\",\n",
    "        \"eval_file\": f\"001_gender_{split}.csv\",\n",
    "        \"clip_seconds\": clip,\n",
    "    }\n",
    "    y_true, y_pred = eval_model(config)\n",
    "\n",
    "    results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: speaker ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f0acc4cba1c49a4a\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-f0acc4cba1c49a4a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b9906ebbe044ffb9b009ab83503c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-f0acc4cba1c49a4a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-9b1013a4ed972841.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b8bc0cbda547b986a8bd259998530d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c0c2a995251c9abb\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-c0c2a995251c9abb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812d40a473db42baa3f6213c09f4a30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-c0c2a995251c9abb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-eb3599ed6b9a654e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d316f5ca4a41fd87c9f4e317ec6c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f0acc4cba1c49a4a\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-f0acc4cba1c49a4a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df77d1df86c94e4aa9520e1536ceefd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-f0acc4cba1c49a4a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-9b1013a4ed972841.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e220e9ca7d4b2cb2c45359082a548f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c0c2a995251c9abb\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-c0c2a995251c9abb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec955c432fd4bebae43be91b168e5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-c0c2a995251c9abb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-eb3599ed6b9a654e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53b3b8d99b843f6bb03d1cd1b91f7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data=results).to_json(\"008_results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "clip = 2\n",
    "OUTPUT_DIR = \"models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_SPEAKER_ID_\" \n",
    "if clip == 2:\n",
    "    OUTPUT_DIR += \"2s_\"\n",
    "\n",
    "for split in \"dev test\".split():\n",
    "    config = {\n",
    "        \"output_column\": \"Speaker_name\",\n",
    "        \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-1250\",\n",
    "        \"eval_file\": f\"003_speaker_id_{split}_for_datasets.csv\",\n",
    "        \"clip_seconds\": clip,\n",
    "    }\n",
    "    y_true, y_pred = eval_model(config)\n",
    "\n",
    "    results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(data=results).to_json(\"008_results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "clip = -1\n",
    "OUTPUT_DIR = \"models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_SPEAKER_ID_\" \n",
    "if clip == 2:\n",
    "    OUTPUT_DIR += \"2s_\"\n",
    "\n",
    "for split in \"dev test\".split():\n",
    "    config = {\n",
    "        \"output_column\": \"Speaker_name\",\n",
    "        \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-1250\",\n",
    "        \"eval_file\": f\"003_speaker_id_{split}_for_datasets.csv\",\n",
    "        \"clip_seconds\": clip,\n",
    "    }\n",
    "    y_true, y_pred = eval_model(config)\n",
    "\n",
    "    results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=results)\n",
    "df.to_json(\"008_results.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Age group - males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-00a104b1622251f1\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc18480f3374572a763c0e1ee10d4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classification problem with 2 classes: ['old', 'young']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:52: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-326d1a891392d841.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-7b7e73aff4c75759.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-16b442975855102d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-0d15654ffbf94999.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-1dc9231538a7e898.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-60eb7132a92114d9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-2fc38c49b55e46fc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/peterr/.cache/huggingface/datasets/csv/default-00a104b1622251f1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-b9290a03a8a3e688.arrow\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-slavic-voxpopuli-v2 were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.weight', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.bias', 'project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-slavic-voxpopuli-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1200\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 2250\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m5roop\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/peterr/macocu/task11/wandb/run-20220517_085622-28u0pead</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/5roop/huggingface/runs/28u0pead\" target=\"_blank\">models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_</a></strong> to <a href=\"https://wandb.ai/5roop/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 2:56:42, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.578200</td>\n",
       "      <td>0.691907</td>\n",
       "      <td>0.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.451500</td>\n",
       "      <td>0.457311</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.585318</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.189200</td>\n",
       "      <td>0.701065</td>\n",
       "      <td>0.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>1.198807</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>1.235574</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>1.362033</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>1.696417</td>\n",
       "      <td>0.712000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.938895</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>1.806149</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>1.487046</td>\n",
       "      <td>0.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.693003</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.044315</td>\n",
       "      <td>0.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>1.942335</td>\n",
       "      <td>0.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>1.934387</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.112688</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>2.515285</td>\n",
       "      <td>0.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>2.308997</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>2.339244</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>2.365810</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>2.319930</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>2.298612</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-150\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-150/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-150/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-150/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-300\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-300/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-150] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-450\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-450/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-450/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-450/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-600\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-600/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-450] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-750\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-750/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-750/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-750/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-900\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-900/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-900/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-900/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-750] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1050\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1050/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1050/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1050/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1200\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1200/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1050] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1350\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1350/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1350/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1350/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1500\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1500/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1500/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1350] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1650\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1650/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1650/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1650/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1800\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1800/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1650] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1950\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1950/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1950/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1950/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2100\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2100/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2100/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2100/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-1950] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count. If Speaker_age_group, Speaker_name, path, Speaker_gender, Speaker_age_at_recording, Count are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250\n",
      "Configuration saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/config.json\n",
      "Model weights saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/pytorch_model.bin\n",
      "Feature extractor saved in models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/preprocessor_config.json\n",
      "Deleting older checkpoint [models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2100] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using custom data configuration default-8cfce226579ef147\n",
      "Reusing dataset csv (/home/peterr/.cache/huggingface/datasets/csv/default-8cfce226579ef147/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e03c7d9e0448a487a115a9e705b137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"old\",\n",
      "    \"1\": \"young\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"old\": 0,\n",
      "    \"young\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.075,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading feature extractor configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"old\",\n",
      "    \"1\": \"young\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"old\": 0,\n",
      "    \"young\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.075,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/vocab.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/tokenizer_config.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/added_tokens.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/special_tokens_map.json. We won't load it.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:52: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n",
      "loading feature extractor configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/vocab.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/tokenizer_config.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/added_tokens.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./added_tokens.json. We won't load it.\n",
      "Didn't find file ./special_tokens_map.json. We won't load it.\n",
      "loading file ./vocab.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-slavic-voxpopuli-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"old\",\n",
      "    \"1\": \"young\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"old\": 0,\n",
      "    \"young\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.075,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForSpeechClassification.\n",
      "\n",
      "All the weights of Wav2Vec2ForSpeechClassification were initialized from the model checkpoint at models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForSpeechClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc233e43185e4a8aaea6889703ef344e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3a190d465e4d698b8a6a2c8e1ff385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d746868b9fefedca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/peterr/.cache/huggingface/datasets/csv/default-d746868b9fefedca/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6a4fdfe9f24be3883be8f7397a2f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06de7355ac549f6a09fc952108f90ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/peterr/.cache/huggingface/datasets/csv/default-d746868b9fefedca/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eae90efb01a458f9c4814cdd2557d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"old\",\n",
      "    \"1\": \"young\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"old\": 0,\n",
      "    \"young\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.075,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading feature extractor configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"old\",\n",
      "    \"1\": \"young\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"old\": 0,\n",
      "    \"young\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.075,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/vocab.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/tokenizer_config.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/added_tokens.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/special_tokens_map.json. We won't load it.\n",
      "loading feature extractor configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/vocab.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/tokenizer_config.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/added_tokens.json. We won't load it.\n",
      "Didn't find file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./added_tokens.json. We won't load it.\n",
      "Didn't find file ./special_tokens_map.json. We won't load it.\n",
      "loading file ./vocab.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-slavic-voxpopuli-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForSpeechClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"old\",\n",
      "    \"1\": \"young\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"old\": 0,\n",
      "    \"young\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.075,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForSpeechClassification.\n",
      "\n",
      "All the weights of Wav2Vec2ForSpeechClassification were initialized from the model checkpoint at models/facebook_wav2vec2-large-slavic-voxpopuli-v2_NEW_AGE_ID_/checkpoint-2250.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForSpeechClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a655cdbb48de444fb6c42a16df751f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e1668b393a4ab0b4145dbc370cfb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import train_model\n",
    "for clip in [-1]:\n",
    "    train_config = {\n",
    "        'model_name_or_path': 'facebook/wav2vec2-large-slavic-voxpopuli-v2',\n",
    "        'TASK': 'NEW_AGE_ID_2s' if clip==2 else \"NEW_AGE_ID\",\n",
    "        'NUM_EPOCH': 15,\n",
    "        'data_files': {\n",
    "            'train': '006_age_train.csv',\n",
    "            'validation': '006_age_test.csv'\n",
    "            },\n",
    "        \"clip_seconds\": clip,\n",
    "        \"output_column\": \"Speaker_age_group\"\n",
    "    }\n",
    "\n",
    "    OUTPUT_DIR = train_model(train_config)\n",
    "    \n",
    "    for split in \"dev test\".split():\n",
    "        config = {\n",
    "            \"output_column\": \"Speaker_age_group\",\n",
    "            \"model_name_or_path\": OUTPUT_DIR+\"/checkpoint-2250\",\n",
    "            \"eval_file\": f\"006_age_{split}.csv\",\n",
    "            \"clip_seconds\": clip,\n",
    "            \"train_config\": train_config,\n",
    "        }\n",
    "        y_true, y_pred = eval_model(config)\n",
    "\n",
    "        results.append({**config, \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(data=results)\n",
    "        df.to_json(\"008_results.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=results)\n",
    "df.to_json(\"008_results.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
